{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompt Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Rishika\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\Rishika\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import torch  # For tensor computations\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification  # To load the tokenizer and model for prompt classification\n",
    "\n",
    "# Loading the pre-trained tokenizer and model for prompt classification\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"rishika0704/promptClassifcation\")  # Load the tokenizer for tokenizing the input prompt\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"rishika0704/promptClassifcation\")  # Load the pre-trained classification model\n",
    "\n",
    "# Defining the possible class labels for prompt classification\n",
    "class_labels = [\"description\", \"explanation\", \"definition\", \"comparison\"]  # Class labels corresponding to the model's output classes\n",
    "\n",
    "# Function to classify the input prompt\n",
    "def classify_prompt(prompt):\n",
    "    # Tokenizing the input prompt into tensors (input format for the model)\n",
    "    # 'pt' ensures PyTorch tensors are used, 'padding' and 'truncation' ensure the prompt fits the required input size\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    \n",
    "    # Disabling gradient calculation as we are in inference mode (no training)\n",
    "    with torch.no_grad():\n",
    "        # Passing the tokenized inputs through the model to get predictions (logits)\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Extracting the raw logits (un-normalized scores) from the model's output\n",
    "    logits = outputs.logits\n",
    "    \n",
    "    # Finding the index of the highest score (predicted class) from the logits\n",
    "    predicted_class = torch.argmax(logits, dim=1).item()\n",
    "    \n",
    "    # Returning the class label corresponding to the predicted index\n",
    "    return class_labels[predicted_class]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The classification of the prompt is 'definition'\n"
     ]
    }
   ],
   "source": [
    "# testing the above code \n",
    "prompt = input(\"Enter a prompt : \")\n",
    "prompt_classification = classify_prompt(prompt)\n",
    "print(f\"The classification of the prompt is '{prompt_classification}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting the keyword from the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rake_nltk import Rake\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Initialize RAKE with NLTK stop words for English\n",
    "rake = Rake(stopwords=stopwords.words('english'))\n",
    "\n",
    "# Define a list of words/terms to filter out if they appear in the extracted keywords\n",
    "filter_words = [\"explain\", \"describe\", \"define\", \"definition\", \"difference\"]\n",
    "\n",
    "def extract_keywords(prompt):\n",
    "    \"\"\"\n",
    "    Extract keywords from the given prompt using RAKE, and filter out any keywords\n",
    "    that contain the terms like 'explain', 'describe', 'define', etc.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Use RAKE to extract keywords from the text\n",
    "    rake.extract_keywords_from_text(prompt)\n",
    "    \n",
    "    # Step 2: Get the ranked keywords/phrases extracted by RAKE\n",
    "    keywords = rake.get_ranked_phrases()\n",
    "\n",
    "    # Step 3: Filter out any keywords that contain unwanted words\n",
    "    # Check if any filter word (from filter_words) appears in the keyword, ignoring case\n",
    "    filtered_keywords = [kw for kw in keywords if not any(fw in kw.lower() for fw in filter_words)]\n",
    "    \n",
    "    # Step 4: Return the filtered list of keywords\n",
    "    return filtered_keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywords from the prompt : ['natural language processing', 'aritificial intelligence']\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Explain me the difference between natural language processing and aritificial intelligence\"\n",
    "keywords = extract_keywords(prompt)\n",
    "print(f\"Keywords from the prompt : {keywords}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Web scraping for the relevant words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipediaapi\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "def get_wikipedia_url(keyword):\n",
    "    \"\"\"\n",
    "    Fetch the Wikipedia URL for a given keyword using the Wikipedia API.\n",
    "    \n",
    "    Args:\n",
    "        keyword (str): The keyword to search for on Wikipedia.\n",
    "        \n",
    "    Returns:\n",
    "        str: The full URL of the Wikipedia page, or None if the page does not exist.\n",
    "    \"\"\"\n",
    "    # Create a Wikipedia API object\n",
    "    wiki_wiki = wikipediaapi.Wikipedia(\"MyProject\", 'en')\n",
    "    # Fetch the Wikipedia page for the keyword\n",
    "    page = wiki_wiki.page(keyword)\n",
    "\n",
    "    # Check if the page exists\n",
    "    if page.exists():\n",
    "        return page.fullurl\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def scrape_wikipedia_terms(keyword):\n",
    "    \"\"\"\n",
    "    Scrape headings from the Wikipedia page of a given keyword.\n",
    "    \n",
    "    Args:\n",
    "        keyword (str): The keyword to scrape from Wikipedia.\n",
    "        \n",
    "    Returns:\n",
    "        list: A list of relevant headings from the Wikipedia page.\n",
    "    \"\"\"\n",
    "    # Get the Wikipedia URL for the given keyword\n",
    "    url = get_wikipedia_url(keyword)\n",
    "\n",
    "    # If no valid URL is found, return an empty list\n",
    "    if url is None:\n",
    "        return []\n",
    "\n",
    "    # Fetch the page content\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find all headings (h2 tags in this case)\n",
    "    paragraphs = soup.find_all('h2')\n",
    "\n",
    "    # Extract text from headings and filter out irrelevant ones\n",
    "    keywords = [para.get_text() for para in paragraphs if para.get_text() not in ['Overview', 'Contents', 'See also', 'References', 'External links', 'Further reading']]\n",
    "\n",
    "    return keywords\n",
    "\n",
    "\n",
    "def get_keywords_for_all(keywords):\n",
    "    \"\"\"\n",
    "    Find keywords for all given keywords and aggregate them into a single list.\n",
    "    \n",
    "    Args:\n",
    "        keywords (list): A list of keywords to scrape.\n",
    "        \n",
    "    Returns:\n",
    "        list: A combined list of keywords scraped from Wikipedia pages.\n",
    "    \"\"\"\n",
    "    all_keywords = []  # Initialize a list to hold all keywords\n",
    "\n",
    "    # Iterate over each keyword and scrape terms\n",
    "    for keyword in keywords:\n",
    "        scraped_keywords = scrape_wikipedia_terms(keyword)  # Get keywords for the current keyword\n",
    "        all_keywords.extend(scraped_keywords)  # Add the scraped keywords to the overall list\n",
    "    all_keywords = list(set(all_keywords))\n",
    "        # print(all_keywords)\n",
    "    return all_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Explain me the difference between natural language processing and artificial intelligence\"\n",
    "keywords = extract_keywords(prompt)\n",
    "keywordsFromNet = get_keywords_for_all(keywords)\n",
    "print(f\"All the keywords from the internet : {keywordsFromNet}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Refining the prompt in a manual way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import requests \n",
    "\n",
    "def generate_refined_prompt(prompt, classification, keywords, related_terms):\n",
    "    \"\"\"\n",
    "    Generate a refined prompt based on the classification, keywords, and related terms.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The initial user prompt.\n",
    "        classification (str): The classification of the prompt (e.g., description, explanation).\n",
    "        keywords (list): List of extracted keywords.\n",
    "        related_terms (list): List of related terms from Wikipedia.\n",
    "\n",
    "    Returns:\n",
    "        str: A refined prompt.\n",
    "    \"\"\"\n",
    "    # Join all keywords into a string for use in the prompt\n",
    "    all_keywords = ', '.join(keywords)\n",
    "\n",
    "    if len(related_terms) == 0:\n",
    "        # Generate prompts based on classification without related terms\n",
    "        if classification == \"description\":\n",
    "            return f\"Can you describe in detail {all_keywords}?\"\n",
    "        elif classification == \"explanation\":\n",
    "            return f\"Can you explain how {all_keywords} work?\"\n",
    "        elif classification == \"definition\":\n",
    "            return f\"Please provide clear definitions of {all_keywords}.\"\n",
    "        elif classification == \"comparison\":\n",
    "            return f\"Can you compare {all_keywords}?\"\n",
    "    else:\n",
    "        # Generate prompts based on classification with related terms\n",
    "        if classification == \"description\":\n",
    "            return f\"Can you describe in detail {all_keywords} and their related terms: {', '.join(related_terms)}?\"\n",
    "        elif classification == \"explanation\":\n",
    "            return f\"Can you explain how {all_keywords} work and their relationships to {', '.join(related_terms)}?\"\n",
    "        elif classification == \"definition\":\n",
    "            return f\"Please provide clear definitions of {all_keywords}, including {', '.join(related_terms)}.\"\n",
    "        elif classification == \"comparison\":\n",
    "            return f\"Can you compare {all_keywords} with {', '.join(related_terms)}?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual Prompt : \n",
      "Please provide clear definitions of natural language processing, artificial intelligence, including Applications, In fiction, Explanatory notes, Common NLP tasks, Future, Ethics, Philosophy, Goals, General tendencies and (possible) future directions, Approaches: Symbolic, statistical, neural networks, History, Techniques.\n"
     ]
    }
   ],
   "source": [
    "manual_prompt = generate_refined_prompt(prompt, prompt_classification, keywords, keywordsFromNet)\n",
    "print(f\"Manual Prompt : \\n{manual_prompt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompt refining from Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the Google Generative AI with the provided API key\n",
    "genai.configure(api_key=\"AIzaSyD0O-LCb0wL4Q7ESz13IM_gXfNaYuxm_yw\")\n",
    "\n",
    "def send_to_gemini(initial_prompt, extracted_keywords):\n",
    "    \"\"\"\n",
    "    Send the initial prompt and extracted keywords to the Gemini API for refinement.\n",
    "\n",
    "    Args:\n",
    "        initial_prompt (str): The initial user prompt.\n",
    "        extracted_keywords (list): The list of keywords extracted from the prompt.\n",
    "\n",
    "    Returns:\n",
    "        str: The refined prompt received from the Gemini API or an error message.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize the Gemini model\n",
    "        model = genai.GenerativeModel('gemini-pro')\n",
    "        response = model.generate_content(f\"\"\"You are a prompt engineer; your task is to refine and enhance the initial prompt so that the output of the refined prompt should be better while staying true to the intent of the initial prompt. Use the important keywords to refine the initial prompt. \n",
    "                                            initial_prompt = {initial_prompt}\n",
    "                                            extracted_keywords = {extracted_keywords}\n",
    "                                            refined_prompt =\"\"\")\n",
    "        return response.text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        # Handle API call errors\n",
    "        return \"Error occurred while contacting the Gemini API.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini Prompt : \n",
      "Provide a detailed explanation of the key differences between natural language processing (NLP) and artificial intelligence (AI), highlighting their applications, common tasks, and future directions.\n"
     ]
    }
   ],
   "source": [
    "gemini_prompt = send_to_gemini(prompt, keywordsFromNet)\n",
    "print(f\"Gemini Prompt : \\n{gemini_prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
